{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62aedfa9",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (ANN) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available. Will install later if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da10f02",
   "metadata": {},
   "source": [
    "## Part 1: Building ANN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        s = ActivationFunctions.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_derivative(x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('Activation Functions and Their Derivatives')\n",
    "\n",
    "axes[0,0].plot(x, ActivationFunctions.sigmoid(x), 'b-', label='Sigmoid')\n",
    "axes[0,0].plot(x, ActivationFunctions.sigmoid_derivative(x), 'r--', label='Derivative')\n",
    "axes[0,0].set_title('Sigmoid')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "axes[0,1].plot(x, ActivationFunctions.relu(x), 'b-', label='ReLU')\n",
    "axes[0,1].plot(x, ActivationFunctions.relu_derivative(x), 'r--', label='Derivative')\n",
    "axes[0,1].set_title('ReLU')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "axes[1,0].plot(x, ActivationFunctions.tanh(x), 'b-', label='Tanh')\n",
    "axes[1,0].plot(x, ActivationFunctions.tanh_derivative(x), 'r--', label='Derivative')\n",
    "axes[1,0].set_title('Tanh')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "axes[1,1].plot(x, ActivationFunctions.linear(x), 'b-', label='Linear')\n",
    "axes[1,1].plot(x, ActivationFunctions.linear_derivative(x), 'r--', label='Derivative')\n",
    "axes[1,1].set_title('Linear')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2692f6c",
   "metadata": {},
   "source": [
    "### Neural Network Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers, activation='sigmoid', learning_rate=0.01):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_layers = len(layers)\n",
    "        \n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = ActivationFunctions.sigmoid\n",
    "            self.activation_derivative = ActivationFunctions.sigmoid_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activation = ActivationFunctions.relu\n",
    "            self.activation_derivative = ActivationFunctions.relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = ActivationFunctions.tanh\n",
    "            self.activation_derivative = ActivationFunctions.tanh_derivative\n",
    "        else:\n",
    "            self.activation = ActivationFunctions.sigmoid\n",
    "            self.activation_derivative = ActivationFunctions.sigmoid_derivative\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n",
    "            b = np.zeros((1, layers[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        self.activations = []\n",
    "        self.z_values = []\n",
    "        \n",
    "        self.cost_history = []\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        \n",
    "        current_input = X\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            if i == self.num_layers - 2 and self.layers[-1] == 1:\n",
    "                activation = z\n",
    "            else:\n",
    "                activation = self.activation(z)\n",
    "            \n",
    "            self.activations.append(activation)\n",
    "            current_input = activation\n",
    "        \n",
    "        return current_input\n",
    "    \n",
    "    def compute_cost(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        if self.layers[-1] == 1:\n",
    "            cost = np.mean((y_true - y_pred) ** 2)\n",
    "        else:\n",
    "            epsilon = 1e-15\n",
    "            y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "            cost = -np.mean(y_true * np.log(y_pred_clipped))\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        dW = [np.zeros_like(w) for w in self.weights]\n",
    "        db = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        if self.layers[-1] == 1:\n",
    "            delta = self.activations[-1] - y\n",
    "        else:\n",
    "            delta = self.activations[-1] - y\n",
    "        \n",
    "        for i in range(self.num_layers - 2, -1, -1):\n",
    "            dW[i] = np.dot(self.activations[i].T, delta) / m\n",
    "            db[i] = np.mean(delta, axis=0, keepdims=True)\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activation_derivative(self.z_values[i-1])\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def update_parameters(self, dW, db):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * dW[i]\n",
    "            self.biases[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward_propagation(X)\n",
    "            \n",
    "            cost = self.compute_cost(y, y_pred)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            dW, db = self.backward_propagation(X, y)\n",
    "            \n",
    "            self.update_parameters(dW, db)\n",
    "            \n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Cost: {cost:.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward_propagation(X)\n",
    "    \n",
    "    def predict_classes(self, X):\n",
    "        predictions = self.predict(X)\n",
    "        if self.layers[-1] == 1:\n",
    "            return (predictions > 0.5).astype(int)\n",
    "        else:\n",
    "            return np.argmax(predictions, axis=1)\n",
    "\n",
    "print(\"Neural Network class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde9a07",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Creation and Examples\n",
    "\n",
    "Let's create some synthetic datasets to test our neural network implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Binary Classification Dataset...\")\n",
    "X_binary, y_binary = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler_binary = StandardScaler()\n",
    "X_binary = scaler_binary.fit_transform(X_binary)\n",
    "y_binary = y_binary.reshape(-1, 1)\n",
    "\n",
    "print(\"Creating Multi-class Classification Dataset...\")\n",
    "X_multi, y_multi = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler_multi = StandardScaler()\n",
    "X_multi = scaler_multi.fit_transform(X_multi)\n",
    "\n",
    "y_multi_onehot = np.zeros((len(y_multi), 3))\n",
    "for i, label in enumerate(y_multi):\n",
    "    y_multi_onehot[i, label] = 1\n",
    "\n",
    "print(\"Creating Regression Dataset...\")\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=1,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler_reg_X = StandardScaler()\n",
    "scaler_reg_y = StandardScaler()\n",
    "X_reg = scaler_reg_X.fit_transform(X_reg)\n",
    "y_reg = scaler_reg_y.fit_transform(y_reg.reshape(-1, 1))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "scatter = axes[0].scatter(X_binary[:, 0], X_binary[:, 1], c=y_binary.ravel(), \n",
    "                         cmap='viridis', alpha=0.7)\n",
    "axes[0].set_title('Binary Classification Dataset')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter, ax=axes[0])\n",
    "\n",
    "scatter = axes[1].scatter(X_multi[:, 0], X_multi[:, 1], c=y_multi, \n",
    "                         cmap='viridis', alpha=0.7)\n",
    "axes[1].set_title('Multi-class Classification Dataset')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter, ax=axes[1])\n",
    "\n",
    "axes[2].scatter(X_reg, y_reg, alpha=0.7)\n",
    "axes[2].set_title('Regression Dataset')\n",
    "axes[2].set_xlabel('Feature')\n",
    "axes[2].set_ylabel('Target')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Binary classification: {X_binary.shape[0]} samples, {X_binary.shape[1]} features\")\n",
    "print(f\"Multi-class classification: {X_multi.shape[0]} samples, {X_multi.shape[1]} features, {len(np.unique(y_multi))} classes\")\n",
    "print(f\"Regression: {X_reg.shape[0]} samples, {X_reg.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24526ca7",
   "metadata": {},
   "source": [
    "## Part 3: Training and Evaluating Custom ANN\n",
    "\n",
    "### Example 1: Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Neural Network for Binary Classification\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "nn_binary = NeuralNetwork([2, 8, 4, 1], activation='sigmoid', learning_rate=0.1)\n",
    "\n",
    "nn_binary.train(X_train, y_train, epochs=1000, verbose=True)\n",
    "\n",
    "y_pred_train = nn_binary.predict(X_train)\n",
    "y_pred_test = nn_binary.predict(X_test)\n",
    "\n",
    "y_pred_train_classes = (y_pred_train > 0.5).astype(int)\n",
    "y_pred_test_classes = (y_pred_test > 0.5).astype(int)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train_classes)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test_classes)\n",
    "\n",
    "print(f\"\\nBinary Classification Results:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(nn_binary.cost_history)\n",
    "axes[0].set_title('Training Loss Curve')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "h = 0.02\n",
    "x_min, x_max = X_binary[:, 0].min() - 1, X_binary[:, 0].max() + 1\n",
    "y_min, y_max = X_binary[:, 1].min() - 1, X_binary[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = nn_binary.predict(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "scatter = axes[1].scatter(X_binary[:, 0], X_binary[:, 1], c=y_binary.ravel(), \n",
    "                         cmap='viridis', edgecolors='black')\n",
    "axes[1].set_title('Decision Boundary')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf844c",
   "metadata": {},
   "source": [
    "### Example 2: Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Neural Network for Multi-class Classification\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multi, y_multi_onehot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "nn_multi = NeuralNetwork([2, 10, 6, 3], activation='sigmoid', learning_rate=0.1)\n",
    "\n",
    "nn_multi.train(X_train_multi, y_train_multi, epochs=1000, verbose=True)\n",
    "\n",
    "y_pred_train_multi = nn_multi.predict(X_train_multi)\n",
    "y_pred_test_multi = nn_multi.predict(X_test_multi)\n",
    "\n",
    "y_pred_train_multi_classes = np.argmax(y_pred_train_multi, axis=1)\n",
    "y_pred_test_multi_classes = np.argmax(y_pred_test_multi, axis=1)\n",
    "y_train_multi_classes = np.argmax(y_train_multi, axis=1)\n",
    "y_test_multi_classes = np.argmax(y_test_multi, axis=1)\n",
    "\n",
    "train_accuracy_multi = accuracy_score(y_train_multi_classes, y_pred_train_multi_classes)\n",
    "test_accuracy_multi = accuracy_score(y_test_multi_classes, y_pred_test_multi_classes)\n",
    "\n",
    "print(f\"\\nMulti-class Classification Results:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_multi:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_multi:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_multi_classes, y_pred_test_multi_classes))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(nn_multi.cost_history)\n",
    "axes[0].set_title('Multi-class Training Loss Curve')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "h = 0.02\n",
    "x_min, x_max = X_multi[:, 0].min() - 1, X_multi[:, 0].max() + 1\n",
    "y_min, y_max = X_multi[:, 1].min() - 1, X_multi[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z_multi = nn_multi.predict(mesh_points)\n",
    "Z_multi = np.argmax(Z_multi, axis=1)\n",
    "Z_multi = Z_multi.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z_multi, alpha=0.3, cmap='viridis')\n",
    "scatter = axes[1].scatter(X_multi[:, 0], X_multi[:, 1], c=y_multi, \n",
    "                         cmap='viridis', edgecolors='black')\n",
    "axes[1].set_title('Multi-class Decision Boundary')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5718169",
   "metadata": {},
   "source": [
    "### Example 3: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Neural Network for Regression\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "nn_reg = NeuralNetwork([1, 8, 4, 1], activation='relu', learning_rate=0.01)\n",
    "\n",
    "nn_reg.train(X_train_reg, y_train_reg, epochs=1000, verbose=True)\n",
    "\n",
    "y_pred_train_reg = nn_reg.predict(X_train_reg)\n",
    "y_pred_test_reg = nn_reg.predict(X_test_reg)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)\n",
    "test_mse = mean_squared_error(y_test_reg, y_pred_test_reg)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(f\"\\nRegression Results:\")\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(nn_reg.cost_history)\n",
    "axes[0].set_title('Regression Training Loss Curve')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].grid(True)\n",
    "\n",
    "sort_idx = np.argsort(X_test_reg.ravel())\n",
    "X_test_sorted = X_test_reg[sort_idx]\n",
    "y_test_sorted = y_test_reg[sort_idx]\n",
    "y_pred_sorted = y_pred_test_reg[sort_idx]\n",
    "\n",
    "axes[1].scatter(X_test_reg, y_test_reg, alpha=0.6, label='Actual', color='blue')\n",
    "axes[1].plot(X_test_sorted, y_pred_sorted, color='red', linewidth=2, label='Predicted')\n",
    "axes[1].set_title('Regression: Actual vs Predicted')\n",
    "axes[1].set_xlabel('Feature')\n",
    "axes[1].set_ylabel('Target')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1968e",
   "metadata": {},
   "source": [
    "## Part 4: ANN Implementation using TensorFlow/Keras\n",
    "\n",
    "Let's implement the same problems using TensorFlow/Keras for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e73bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing TensorFlow...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow\"])\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    print(f\"TensorFlow installed successfully! Version: {tf.__version__}\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f6ece",
   "metadata": {},
   "source": [
    "### Binary Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Keras Neural Network for Binary Classification\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "model_binary = keras.Sequential([\n",
    "    layers.Dense(8, activation='sigmoid', input_shape=(2,)),\n",
    "    layers.Dense(4, activation='sigmoid'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_binary.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_binary.summary()\n",
    "\n",
    "history_binary = model_binary.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "train_loss, train_acc = model_binary.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_acc = model_binary.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nKeras Binary Classification Results:\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(history_binary.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history_binary.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('Keras Binary Classification - Loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history_binary.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history_binary.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_title('Keras Binary Classification - Accuracy')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617c71a",
   "metadata": {},
   "source": [
    "### Multi-class Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Keras Neural Network for Multi-class Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_multi = keras.Sequential([\n",
    "    layers.Dense(10, activation='relu', input_shape=(2,)),\n",
    "    layers.Dense(6, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model_multi.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_multi.summary()\n",
    "\n",
    "history_multi = model_multi.fit(\n",
    "    X_train_multi, y_train_multi,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "train_loss_multi, train_acc_multi = model_multi.evaluate(X_train_multi, y_train_multi, verbose=0)\n",
    "test_loss_multi, test_acc_multi = model_multi.evaluate(X_test_multi, y_test_multi, verbose=0)\n",
    "\n",
    "print(f\"\\nKeras Multi-class Classification Results:\")\n",
    "print(f\"Training Accuracy: {train_acc_multi:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_multi:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(history_multi.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history_multi.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('Keras Multi-class Classification - Loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history_multi.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history_multi.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_title('Keras Multi-class Classification - Accuracy')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ba382",
   "metadata": {},
   "source": [
    "## Part 5: Real-world Example - Iris Dataset\n",
    "\n",
    "Let's apply our ANN implementations to the classic Iris dataset for flower classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Iris Dataset\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "iris_df = pd.DataFrame(X_iris, columns=iris.feature_names)\n",
    "iris_df['species'] = [iris.target_names[i] for i in y_iris]\n",
    "\n",
    "print(\"Dataset Shape:\", X_iris.shape)\n",
    "print(\"Number of classes:\", len(iris.target_names))\n",
    "print(\"Class names:\", iris.target_names)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(iris_df.head())\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(iris_df.describe())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "features = iris.feature_names\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(features):\n",
    "        for j, species in enumerate(iris.target_names):\n",
    "            mask = y_iris == j\n",
    "            ax.hist(X_iris[mask, i], alpha=0.7, label=species, bins=20)\n",
    "        ax.set_title(features[i])\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = y_iris == i\n",
    "    plt.scatter(X_iris[mask, 0], X_iris[mask, 1], \n",
    "               c=colors[i], label=species, alpha=0.7, s=50)\n",
    "\n",
    "plt.xlabel(features[0])\n",
    "plt.ylabel(features[1])\n",
    "plt.title('Iris Dataset: Sepal Length vs Sepal Width')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c7905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_iris = StandardScaler()\n",
    "X_iris_scaled = scaler_iris.fit_transform(X_iris)\n",
    "\n",
    "y_iris_onehot = np.zeros((len(y_iris), 3))\n",
    "for i, label in enumerate(y_iris):\n",
    "    y_iris_onehot[i, label] = 1\n",
    "\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris_scaled, y_iris_onehot, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "y_train_iris_labels = np.argmax(y_train_iris, axis=1)\n",
    "y_test_iris_labels = np.argmax(y_test_iris, axis=1)\n",
    "\n",
    "print(\"Iris Dataset Preparation:\")\n",
    "print(f\"Training set: {X_train_iris.shape}\")\n",
    "print(f\"Test set: {X_test_iris.shape}\")\n",
    "print(f\"Features: {X_iris.shape[1]}\")\n",
    "print(f\"Classes: {len(iris.target_names)}\")\n",
    "\n",
    "print(\"\\nTraining Custom Neural Network on Iris Dataset\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "nn_iris = NeuralNetwork([4, 8, 6, 3], activation='sigmoid', learning_rate=0.1)\n",
    "nn_iris.train(X_train_iris, y_train_iris, epochs=1000, verbose=False)\n",
    "\n",
    "y_pred_iris = nn_iris.predict(X_test_iris)\n",
    "y_pred_iris_classes = np.argmax(y_pred_iris, axis=1)\n",
    "\n",
    "custom_accuracy = accuracy_score(y_test_iris_labels, y_pred_iris_classes)\n",
    "print(f\"Custom Neural Network Accuracy: {custom_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nTraining Keras Neural Network on Iris Dataset\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "model_iris = keras.Sequential([\n",
    "    layers.Dense(8, activation='relu', input_shape=(4,)),\n",
    "    layers.Dense(6, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model_iris.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_iris = model_iris.fit(\n",
    "    X_train_iris, y_train_iris_labels,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "keras_loss, keras_accuracy = model_iris.evaluate(X_test_iris, y_test_iris_labels, verbose=0)\n",
    "print(f\"Keras Neural Network Accuracy: {keras_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison on Iris Dataset:\")\n",
    "print(f\"Custom Implementation: {custom_accuracy:.4f}\")\n",
    "print(f\"Keras Implementation:  {keras_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nDetailed Classification Report (Custom NN):\")\n",
    "print(classification_report(y_test_iris_labels, y_pred_iris_classes, \n",
    "                          target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a40ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(nn_iris.cost_history)\n",
    "axes[0, 0].set_title('Custom NN - Training Loss (Iris)')\n",
    "axes[0, 0].set_xlabel('Epochs')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(history_iris.history['loss'], label='Training Loss')\n",
    "axes[0, 1].plot(history_iris.history['val_loss'], label='Validation Loss')\n",
    "axes[0, 1].set_title('Keras NN - Training Loss (Iris)')\n",
    "axes[0, 1].set_xlabel('Epochs')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "models = ['Custom NN', 'Keras NN']\n",
    "accuracies = [custom_accuracy, keras_accuracy]\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "\n",
    "axes[1, 0].bar(models, accuracies, color=colors)\n",
    "axes[1, 0].set_title('Model Accuracy Comparison (Iris)')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_iris_labels, y_pred_iris_classes)\n",
    "im = axes[1, 1].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "axes[1, 1].set_title('Confusion Matrix (Custom NN)')\n",
    "axes[1, 1].set_xlabel('Predicted')\n",
    "axes[1, 1].set_ylabel('Actual')\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        axes[1, 1].text(j, i, str(cm[i, j]), ha='center', va='center')\n",
    "\n",
    "axes[1, 1].set_xticks(range(len(iris.target_names)))\n",
    "axes[1, 1].set_yticks(range(len(iris.target_names)))\n",
    "axes[1, 1].set_xticklabels(iris.target_names, rotation=45)\n",
    "axes[1, 1].set_yticklabels(iris.target_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
